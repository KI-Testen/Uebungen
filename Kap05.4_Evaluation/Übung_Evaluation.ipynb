{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "015c5d66-94be-4251-97c0-5fffe65c80ee",
            "metadata": {},
            "source": [
                "# \u00dcbung zu Kapitel 5.4 - Evaluieren eines erstellten ML-Modells\n",
                "\n",
                "*Eine \u00dcbung zum Buch \"[Basiswissen KI-Testen - Qualit\u00e4t von und mit KI-basierten Systemen](https://dpunkt.de/produkt/basiswissen-ki-testen/)\", ISBN 978-3-86490-947-4*\n",
                "\n",
                "In dieser \u00dcbung evaluieren wir die Modelle, die wir in der [\u00dcbung 4.2](../Kap04.2_Datens\u00e4tze_aufteilen/\u00dcbung_Datens\u00e4tze_aufteilen.ipynb) trainiert haben. Dazu verwenden wir die Testdaten, die wir dort zur\u00fcckgelegt haben. Auch hier nutzen wir wieder mehrere Bibliotheken, die daf\u00fcr passende Methoden bereithalten:\n",
                "\n",
                "[<img src=\"https://pandas.pydata.org/docs/_static/pandas.svg\" alt=\"pandas\" width=\"80\" height=\"24\">](https://pandas.pydata.org/docs/reference/index.html)\n",
                "&emsp; [<img src=\"https://joblib.readthedocs.io/en/stable/_static/joblib_logo.svg\" alt=\"joblib\" width=\"36\" height=\"36\"> joblib](https://joblib.readthedocs.io/en/stable)\n",
                "&emsp; [<img src=\"https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" alt=\"Scikit-learn\" width=\"80\" height=\"24\">](https://scikit-learn.org/stable/modules/classes.html)\n",
                "&emsp; [<img src=\"https://seaborn.pydata.org/_static/logo-wide-lightbg.svg\" alt=\"seaborn\" width=\"80\" height=\"24\">](https://seaborn.pydata.org/index.html)\n",
                "&emsp; [<img src=\"https://matplotlib.org/_static/logo_light.svg\" alt=\"matplotlib\" width=\"80\" height=\"24\">](https://matplotlib.org/)\n",
                "\n",
                "**Aufgabe:** Zur Evaluierung ermittelst du die vier funktionalen Leistungsmetriken. Hier musst du beachten, dass vom Entscheidungsbaum drei Klassen ausgegeben werden. Die Konfusionsmatrix besteht daher aus drei Zeilen und drei Spalten!\n",
                "\n",
                "F\u00fcr diese Aufgabe gehen wir in f\u00fcnf Schritten vor:\n",
                "1. Vorbereitung: Testdaten laden, Modell laden, Konfusionsmatrix ermitteln,\n",
                "1. Genauigkeit (engl. accuracy) evaluieren,\n",
                "1. Pr\u00e4zision (engl. precision) evaluieren,\n",
                "1. Sensitivit\u00e4t (engl. recall) evaluieren,\n",
                "1. F1-Wert (engl. F1-score) evaluieren."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e54fc95a-2e87-4593-8ebf-86f1db42ada6",
            "metadata": {},
            "source": [
                "## 1. Vorbereitung\n",
                "### Laden der Testdaten\n",
                "Als Erstes laden wir im Folgenden die Testdaten. Diese Testdaten haben wir in der [\u00dcbung 4.2](../Kap04.2_Datens\u00e4tze_aufteilen/\u00dcbung_Datens\u00e4tze_aufteilen.ipynb) vom Datensatz abgetrennt und f\u00fcr die Modellevaluierung zur\u00fcckgehalten. Diese Daten haben wir zus\u00e4tzlich im Ordner \"data\" in der Datei \"iris2_testdata.csv\" abgelegt, damit du die \u00dcbungen unabh\u00e4ngig voneinander bearbeiten kannst."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70ef007b-15c4-48c2-8def-1268f514b087",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd         # zum Laden der Daten verwenden wir die pandas Bibliothek\n",
                "X_test = pd.read_csv('../Kap04.2_Datens\u00e4tze_aufteilen/X_test.csv') # wenn du die \u00dcbung 4.2 nicht bis zum Ende abgeschlossen hast, ...\n",
                "y_test = pd.read_csv('../Kap04.2_Datens\u00e4tze_aufteilen/y_test.csv') # ... kannst du die Daten auch aus dem Verzeichnis \"data/\" dieser \u00dcbung laden.\n",
                "#X_test = pd.read_csv('data/X_test.csv') # wenn du die \u00dcbung 4.2 nicht bis zum Ende abgeschlossen hast, ...\n",
                "#y_test = pd.read_csv('data/y_test.csv') # ... kannst du die Daten auch aus dem Verzeichnis \"data/\" dieser \u00dcbung laden."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6eed2494-3393-4747-98dc-e8c7c82fd64e",
            "metadata": {},
            "source": [
                "Wir schauen uns die Testdaten genauer an: "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a864dff5-702c-44b1-9a53-9ed17dc1cdc5",
            "metadata": {},
            "outputs": [],
            "source": [
                "X_test"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6fd355a0-1324-471a-b652-a30304eeea5f",
            "metadata": {},
            "source": [
                "Die dazugeh\u00f6rigen Klassen der 15 Pflanzen in den Testdaten befinden sich in der Variablen y_test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7068b6ee-32d6-4ea7-8b1b-ed33ee549a77",
            "metadata": {},
            "outputs": [],
            "source": [
                "y_test"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df0f2082-f558-48b4-97a3-b1bd131260be",
            "metadata": {},
            "source": [
                "### Laden des trainierten Modells\n",
                "In [\u00dcbung 4.2](../Kap04.2_Datens\u00e4tze_aufteilen/\u00dcbung.ipynb) haben wir einen Entscheidungsbaum mit der Tiefe 8 trainiert. Diesen Entscheidungsbaum haben wir dort unter dem Namen `modell-8.pkl` abgelegt. Diesen wollen wir nun mit Hilfe der [joblib](https://joblib.readthedocs.io/en/stable/) Bibliothek laden und anhand verschiedener Metriken evaluieren. Dazu schauen wir uns die Vorhersage (engl. prediction) des Entscheidungsbaums zu den Testdaten genauer an."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49bdc82e-6592-4b7d-9b54-1f8fb195d190",
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib\n",
                "model_eval = joblib.load(\"../Kap04.2_Datens\u00e4tze_aufteilen/modell-8.pkl\") \n",
                "#model_eval = joblib.load(\"data/Modelle/modell-8.pkl\") # alternatives Modell, wenn du \u00dcbung 4.2 nicht bis zum Ende abgeschlossen hast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c716bd92-9d05-40ca-ab7e-c56b2188f14a",
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model_eval.predict(X_test)\n",
                "df_predictions = pd.DataFrame({'Vorhersagte Klasse': y_pred, 'Tats\u00e4chliche Klasse': y_test['class']})\n",
                "df_predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fe53ec67-ec99-43b2-b8a0-89488f9ec4d7",
            "metadata": {},
            "source": [
                "### Darstellung der Vorhersagen durch das Modell\n",
                "Um einen besseren \u00dcberblick \u00fcber die Richtigkeit der Vorhersagen zu bekommen, stellen wir die Ergebnisse in einer Konfusionsmatrix dar. Daf\u00fcr k\u00f6nnen wir die Funktion [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) aus der sklearn-Unterbibliothek <i>metrics</i> verwenden. F\u00fcr die Darstellung der Konfusionsmatrix verwenden wir die Bibliotheken [matplotlib](https://matplotlib.org/) und [seaborn](https://seaborn.pydata.org/). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a52f4028-8c1b-4d50-8ef8-1d48d62a0308",
            "metadata": {},
            "outputs": [],
            "source": [
                "from   sklearn.metrics import confusion_matrix\n",
                "import seaborn             as sns\n",
                "import matplotlib.pyplot   as plt\n",
                "\n",
                "# Inhalt f\u00fcr Konfusionsmatrix zusammenstellen\n",
                "y_pred = model_eval.predict(X_test)\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "class_names = ['Klasse 0', 'Klasse 1', 'Klasse 2']\n",
                "\n",
                "# Konfusionsmatrix mit Seaborn darstellen\n",
                "plt.figure()\n",
                "sns.heatmap(cm, annot=True, cmap='Blues', cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
                "plt.xlabel('Vorhergesagte Klassen')\n",
                "plt.ylabel('Tats\u00e4chliche Klassen')\n",
                "plt.title('Konfusionsmatrix')\n",
                "plt.show()\n",
                "\n",
                "#Achtung: Hier sind die Achsen anders ansgeordnet als im Buch. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e67ce08-aa7f-4cba-b2e5-ec6cd3cc4d1b",
            "metadata": {},
            "source": [
                "## 2. Genauigkeit evaluieren\n",
                "\n",
                "Berechne die <b>Genauigkeit</b> (engl. accuracy) f\u00fcr das Modell anhand der Test-Daten! \n",
                "Hier m\u00fcssen wir beachten, dass wir nicht nur zwei Klassen haben, sondern 3 Klassen. Daher berechnen wir die Genauigkeit als die Anzahl der richtigen Klassifikationen geteilt durch die Anzahl aller Klassifikationen.\n",
                "\n",
                "Wenn wir auf die Konfusionsmatrix oben schauen, ist dies: (6 + 2 + 6) / 15\n",
                "\n",
                "Wir k\u00f6nnen daf\u00fcr auch die Funktion [accuracy_score(y_true, y_pred, ...)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) aus der sklearn-Bibliothek verwenden."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc70e3c7-290c-48bb-ad6d-0bf4d7d129db",
            "metadata": {
                "tags": [
                    "Aufgabe"
                ]
            },
            "outputs": [],
            "source": [
                "### Genauigkeit (engl. accuracy) = (RP + RN) / (RP + FP + RN + FN)\n",
                "from sklearn.metrics import accuracy_score\n",
                "accuracy = accuracy_score(y_true = ..., y_pred = ...)\n",
                "print(\"Genauigkeit: \", accuracy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4f7cf22-0f08-46a7-82df-53490bd08102",
            "metadata": {
                "tags": [
                    "L\u00f6sung"
                ],
                "jupyter": {
                    "source_hidden": true
                }
            },
            "outputs": [],
            "source": [
                "# Um die L\u00f6sung anzuzeigen, bitte diese Zelle zweimal ausf\u00fchren\n",
                "%load L\u00f6sungen/L\u00f6sung01.py"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47b54730-d08c-42a7-a973-5bf7f3fb897d",
            "metadata": {},
            "source": [
                "## 3. Pr\u00e4zision evaluieren\n",
                "Berechne die **Pr\u00e4zision** (engl. precision) f\u00fcr das Modell! Hier k\u00f6nnen wir die sklearn-Funktion [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) verwenden. Achte darauf, dass wir hier eine mehrklassige Klassifikation haben, und probiere verschiedene Werte f\u00fcr den Parameter `average` aus. Der average-Parameter bestimmt, wie die Pr\u00e4zision f\u00fcr mehrklassige Klassifikationen berechnet wird. Hier gibt es kein \"richtig\" und \"falsch\", die Wahl des Parameters gibt viel mehr verschiedene Antworten auf die Frage nach der Pr\u00e4zision. Schaue dir zumindest das Ergebnis f\u00fcr den *average*-Wert `None` an."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9676b9f1-5011-47ac-9c15-c0f645db9a56",
            "metadata": {
                "tags": [
                    "Aufgabe"
                ]
            },
            "outputs": [],
            "source": [
                "### Pr\u00e4zision (engl. precision) = RP / (RP + FP)  \n",
                "from sklearn.metrics import precision_score\n",
                "\n",
                "precision = precision_score(y_true = ...., y_pred = ..., average = ...)\n",
                "print(\"Pr\u00e4zision: \", precision)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "650dc389-c557-4345-891c-5b4e8ade7fd7",
            "metadata": {
                "tags": [
                    "L\u00f6sung"
                ],
                "jupyter": {
                    "source_hidden": true
                }
            },
            "outputs": [],
            "source": [
                "# Um die L\u00f6sung anzuzeigen, bitte diese Zelle zweimal ausf\u00fchren\n",
                "%load L\u00f6sungen/L\u00f6sung02.py"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3fed6714-701b-4e59-86de-9b7f5e1d20a9",
            "metadata": {},
            "source": [
                "## 4. Sensitivit\u00e4t evaluieren\n",
                "Berechne die **Sensitivit\u00e4t** (engl. recall)! Dazu k\u00f6nnen wir die sklearn-Funktion [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) verwenden. Auch hier kannst du bei der Funktion *reacll_score* f\u00fcr den Parameter `average` verschiedene Werte setzen. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "77d4ff61-b283-4c0c-bfe1-900d006ce7d4",
            "metadata": {
                "tags": [
                    "Aufgabe"
                ]
            },
            "outputs": [],
            "source": [
                "### Sensitivit\u00e4t (engl. recall) = RP / (RP + FN)\n",
                "from sklearn.metrics import recall_score\n",
                "\n",
                "recall = recall_score(y_true = ...., y_pred = ..., average= ...)\n",
                "print(\"Sensitivit\u00e4t: \", recall)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e33ec176-2f2d-4d40-a3a0-97c17ad4532c",
            "metadata": {
                "tags": [
                    "L\u00f6sung"
                ],
                "jupyter": {
                    "source_hidden": true
                }
            },
            "outputs": [],
            "source": [
                "# Um die L\u00f6sung anzuzeigen, bitte diese Zelle zweimal ausf\u00fchren\n",
                "%load L\u00f6sungen/L\u00f6sung03.py"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c5b7b02-8d41-492a-8df2-ce7d3e7a9e73",
            "metadata": {},
            "source": [
                "## 5. F1-Wert evaluieren\n",
                "Berechne den **F1-Wert** (engl. F1-score)! Dazu k\u00f6nnen wir die sklearn-Funktion [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) verwenden. Hier k\u00f6nnen wir wieder wie bei den zwei vorherigen Funktion f\u00fcr den Parameter *average* verschiedene Werte setzen. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8155f8d-6bb3-4482-a2f5-bccdc1a8dc8e",
            "metadata": {
                "tags": [
                    "Aufgabe"
                ]
            },
            "outputs": [],
            "source": [
                "### F1-Wert (engl. F1-score) = 2*(Pr\u00e4zision*Sensitivit\u00e4t) / (Pr\u00e4zision + Sensitivit\u00e4t)\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "f1 = f1_score(y_true = ...., y_pred = ..., average= ...)\n",
                "print(\"F1-Wert: \", f1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b039cf97-1d51-4b0e-8a12-d83f50e2ff72",
            "metadata": {
                "tags": [
                    "L\u00f6sung"
                ],
                "jupyter": {
                    "source_hidden": true
                }
            },
            "outputs": [],
            "source": [
                "# Um die L\u00f6sung anzuzeigen, bitte diese Zelle zweimal ausf\u00fchren\n",
                "%load L\u00f6sungen/L\u00f6sung04.py"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "490460d2-3dc0-4641-9342-955c5194638e",
            "metadata": {},
            "source": [
                "## Aufgabe (optional)\n",
                "Interessant ist, wie sich die verschiedenen Metriken mit steigender Komplexit\u00e4t des Entscheidungsbaums verhalten. Dazu laden wir Variationen des Modells aus der \u00dcbung Kap04.2_Datens\u00e4tze_aufteilen aus dem Verzeichnis *data/Modelle* dieser \u00dcbung und schauen uns in einem Plot an, wie sich die verschiedenen Metriken mit zunehmender Komplexit\u00e4t ver\u00e4ndern."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b64a869c-d68e-420d-af15-cd8e9a88b81b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt   # Wir benutzen die matplot-Bibliothek zum Zeichnen der Grafik\n",
                "import joblib\n",
                "from pathlib import Path   \n",
                "import os\n",
                "\n",
                "# Initialisierung der Listen die wir sp\u00e4ter in der Grafik verwenden werden.\n",
                "accuracy_test  = []\n",
                "precision_test = []\n",
                "recall_test    = [] \n",
                "f1_test        = []\n",
                "model_name     = []\n",
                "\n",
                "# Berechnung der Metriken \u00fcber alle Entscheidungsb\u00e4ume (die wie in \u00dcbung Kap04.2_Datens\u00e4tze_aufteilen trainiert wurden) mit einer For-Schleife \n",
                "dir= Path(\"data/Modelle\")         # in diesem Verzeichnis ...\n",
                "for model in dir.glob('*.pkl'):   # ... suche alle pkl-Dateien\n",
                "    # Laden der Modelle\n",
                "    model_eval = joblib.load(model)\n",
                "    y_pred = model_eval.predict(X_test)\n",
                "    # Berechnung der Metriken\n",
                "    accuracy_test.append(accuracy_score(y_test, y_pred))\n",
                "    precision_test.append(precision_score(y_test, y_pred, average='macro', zero_division=1))\n",
                "    recall_test.append(recall_score(y_test, y_pred, average='macro', zero_division=1))\n",
                "    f1_test.append(f1_score(y_test, y_pred, average='macro', zero_division=1))\n",
                "    # Den Namen des Modells entnehmen und f\u00fcr die sp\u00e4tere x-Achse in die Liste anh\u00e4ngen\n",
                "    model_name.append(model.stem)\n",
                "    \n",
                "# Grafik erzeugen\n",
                "fig, ax1 = plt.subplots() \n",
                "ax1.set_xlabel('Modelle')   # x-Achse\n",
                "ax1.set_ylabel('Metrik')    # y-Achse\n",
                "ax1.plot(model_name, accuracy_test,  label = \"Genauigkeit\")\n",
                "ax1.plot(model_name, precision_test, label = \"Pr\u00e4zision\")\n",
                "ax1.plot(model_name, recall_test,    label = \"Sensitivit\u00e4t\")\n",
                "ax1.plot(model_name, f1_test,        label = \"F1-Wert\")\n",
                "plt.ylim([0, 1])\n",
                "ax1.legend() \n",
                "plt.tight_layout()  # Dies sorgt daf\u00fcr, dass die Plots und Beschriftungen nicht \u00fcberlappen\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
